Architected comprehensive data cleaning framework for quantitative research system





Part 2: Data Cleaning Framework
Role: Quantitative Researcher

Executive Summary
Data cleaning is the critical filtering layer between raw market data and research-grade analysis. Unlike traditional data science where aggressive imputation is common, quantitative finance demands extreme conservatism: preserve market reality, flag ambiguity, never fabricate information. This document establishes a systematic framework for identifying, classifying, and addressing data quality issues while maintaining the temporal integrity and statistical properties essential for unbiased research.

The core principle: Transparency over correction. Exclusion over invention. Documentation over silent fixes.

1. Philosophy of Quantitative Data Cleaning
1.1 Fundamental Principles
Principle 1: Market Data Reflects Reality, Including Imperfection

Missing bars, halts, extreme moves, and anomalous volume are often signal, not noise:

Trading halt → regulatory event or news
Zero volume → illiquidity or technical issue
Extreme spike → flash crash or fat-finger trade
Implication: Aggressive cleaning can destroy the very phenomena we study (tail events, regime transitions, microstructure effects).

Principle 2: Distinguish Data Quality Issues from Market Events

Two fundamentally different categories:

Category	Example	Treatment
Data Error	Negative price, duplicate timestamp, OHLC violation	Correct or exclude
Market Event	Trading halt, 20% spike, circuit breaker	Preserve with metadata
Principle 3: Immutability of Raw Data

Never modify the raw ingestion layer. All cleaning produces derived datasets with full lineage:

raw/ → cleaned/ → features/ → models/
  ↓       ↓
 (preserved) (documented transformations)
Principle 4: Research Context Determines Tolerance

Different research applications require different cleanliness standards:

High-frequency execution models: Zero tolerance for missing bars
Daily volatility research: Can aggregate over gaps
Tail risk analysis: Requires preserving extreme events
Backtesting: Needs conservative exclusion to prevent unrealistic fills
1.2 The Cleaning Paradox
Conservative Cleaning ↔ Survival Bias

Too aggressive → lose rare events → underestimate risk
Too lenient → include bad data → spurious signals
Resolution: Multi-tier cleaning with explicit severity levels:

Critical errors → Always exclude
Suspicious anomalies → Flag, include with warning
Market events → Preserve, enrich with metadata
Ambiguous cases → Default to inclusion + documentation
2. Issue Classification Taxonomy
2.1 Structural Integrity Violations (Critical)
A. Timestamp Issues

Duplicate Timestamps:

Manifestation: Two bars with identical timestamp for same symbol
Root cause: API bug, data vendor error, DST handling
Impact: Breaks time-series indexing, violates uniqueness assumption
Detection Logic:

Group by (symbol, timestamp), count > 1
Check if duplicates have identical OHLCV or differ
Resolution Strategy:

If OHLCV identical: Remove duplicate (likely transmission error)
If OHLCV differs: Cannot determine truth → exclude both bars
Log incident for vendor escalation
Out-of-Order Records:

Manifestation: Bar at t+2 appears before bar at t+1
Root cause: Network reordering, database race condition
Impact: Violates temporal causality assumptions
Resolution:

Sort by timestamp on ingestion
If detected post-ingestion → reprocess entire day (data corruption risk)
B. OHLC Constraint Violations

Mathematical Impossibilities:

Violations:

high < max(open, close)
low > min(open, close)
high < low
Any OHLC value ≤ 0
These are ALWAYS errors (not market events):

Physical impossibility (price can't be negative)
Data transmission corruption
Feed parser bug
Resolution: Exclude bar entirely

Cannot reconstruct true values
Inclusion would violate downstream models
Log for pattern analysis (systematic vendor issue?)
C. Volume Anomalies

Negative Volume:

Physically impossible
Exclude immediately
Zero Volume:

NOT always an error
Common scenarios:
Trading halt continuation
Low liquidity stocks during off-hours
Post-split adjustment artifacts
Treatment:

Preserve in cleaned data
Add metadata flag: volume_zero=True
Researcher decides handling (exclude for liquidity studies, keep for price analysis)
2.2 Statistical Anomalies (Requires Judgment)
A. Extreme Price Moves

Definition Challenge: What constitutes "extreme"?

Context-Dependent Thresholds:

Context	1-Min Return Threshold	Rationale
Large-cap equity	>10%	Extremely rare, likely error or flash crash
Micro-cap equity	>20%	Higher volatility, news sensitivity
Earnings day	>15%	Expect elevated moves
Market crash day	>15%	Systemic event
Two-Stage Detection:

Stage 1: Unconditional Screening

Flag returns > 10% for manual review
Not automatic exclusion
Stage 2: Contextual Validation

For flagged bar:
1. Check if multiple stocks show extreme moves → market-wide event (preserve)
2. Check if move reversed immediately next bar → potential error
3. Check news feeds for symbol → event-driven (preserve)
4. Check if volume = 0 or abnormally low → suspicious
5. Compare to consolidated tape (if available) → verify accuracy
Resolution Hierarchy:

Confirmed data error → Exclude
Confirmed market event → Preserve + flag (extreme_move=True)
Ambiguous → Preserve + flag (needs_review=True)
B. Volume Spikes

Typical Pattern: Volume 50x daily average in single bar

Legitimate Scenarios:

Block trades
News releases
Market open/close
Algorithmic sweep
Suspicious Scenarios:

Volume spike + price unchanged
Volume spike + no contemporaneous news
Volume spike inconsistent with order book depth
Treatment:

Never exclude based on volume alone
Flag outliers: volume_spike=True if > 10σ from intraday mean
Enable researcher filtering
C. Price Discreteness Violations

Tick Size Rules (U.S. Equities):

Price ≥ $1.00 → minimum tick = $0.01
Price < $1.00 → minimum tick = $0.0001
Violation Example: Price = $45.1234567

Root Cause:

Data feed decimalization error
Adjustment calculation artifact
Resolution:

Round to nearest valid tick
This is safe correction (information loss is negligible)
Document rounding in metadata
2.3 Missing Data Patterns
A. Missing Bars (Temporal Gaps)

Classification by Gap Size:

1-2 Bar Gaps:

Common, often benign
API latency, low liquidity
Treatment: Preserve gap, do NOT forward-fill
3-10 Bar Gaps:

Suspicious if during regular hours
May indicate trading halt
Treatment:
Check halt database
Flag as potential_halt=True
Preserve gap
>10 Bar Gaps:

Almost certainly data issue or extended halt
Treatment:
Escalate to data quality review
If halt confirmed → preserve
If data error → mark day as incomplete=True
B. Missing Days

Complete Trading Day Missing:

Critical Decision: Backfill or Exclude?

Backfill when:

Data exists from vendor, was never ingested
Clear ingestion failure (logged)
Can be retrieved with full confidence
Exclude when:

Data not available from any source
Symbol was halted entire day
Corporate action in progress
Treatment:

Mark universe membership: tradeable_days list per symbol
Downstream research filters on this
Prevents look-ahead bias (using stocks that "appeared" later)
C. Partial Session Data

Early Close Days:

Expected: Thanksgiving (1:00 PM close), day before Christmas
Treatment: Normal data, just fewer bars
Store expected close time in metadata
Unexplained Partial Data:

Red flag: Data ends at 2:47 PM on normal trading day
Treatment:
Mark day as incomplete=True
Exclude from research requiring full-day dynamics
Attempt backfill
If persistent → document as known limitation
2.4 Cross-Sectional Anomalies
Correlation Breakdown:

Pattern: Stock typically 0.8 correlated with sector shows -0.5 correlation on single day

Causes:

Stock-specific news
Data error (wrong ticker mapped)
Corporate action misalignment
Detection:

Compute rolling 20-day pairwise correlations
Flag deviations > 3σ
Treatment:

Investigate flagged days
If explainable (earnings, merger announcement) → preserve
If unexplainable + other anomalies → suspicious, flag for review
3. Repair vs. Exclusion Decision Framework
3.1 Decision Matrix
Issue Type	Certainty of Error	Repairability	Action
OHLC violation	100%	Impossible	Exclude
Duplicate timestamp (identical values)	100%	Trivial	Remove duplicate
Duplicate timestamp (different values)	100%	Impossible	Exclude both
Price spike with reversal, zero volume	95%	Uncertain	Exclude
Price spike with high volume, confirmed news	5%	N/A	Preserve
Missing bar (1-2 gap)	50%	Depends	Preserve gap
Missing bar (>10 gap)	90%	Requires backfill	Flag, attempt backfill
Tick size violation	80%	Safe	Round to valid tick
Zero volume during halt	0% (not error)	N/A	Preserve + metadata
3.2 Safe Corrections (Conservative Repairs)
What Can Be Safely Corrected:

1. Precision Standardization

Round prices to valid tick sizes
Standardize timestamp precision (microseconds → milliseconds)
Rationale: Information loss is negligible, prevents numerical instability
2. Timezone Normalization

Convert all timestamps to UTC
Store original timezone in metadata
Rationale: Essential for reproducibility, no data loss
3. Corporate Action Adjustments

Apply split/dividend adjustments consistently
Rationale: Mathematical transformation, not data fabrication
Requirement: Store both raw and adjusted versions
4. Outlier Winsorization (With Extreme Caution)

Cap returns at 99.9th percentile only if:
Confirmed data error (not market event)
Used for parameter estimation (not signal generation)
Documented in methodology
3.3 Dangerous Corrections (Never Perform)
1. Forward Filling Missing Bars

Why tempting: Eliminates gaps, simplifies indexing

Why catastrophic:

Reality:  [100, 101, <HALT>, <HALT>, 95, 96]
Forward:  [100, 101,   101,     101,  95, 96]

Implied return: -5.9% in one bar
Actual: Multiple bars of no trading, then -5.9%

Impact on models:
- Volatility estimation: Compressed (fewer bars to diffuse shock)
- Execution simulation: Unrealistic fills during halt
- Risk models: Underestimate gap risk
Correct Approach:

Preserve gaps
Use gap-aware interpolation only for visualization
Never use interpolated values in model training
2. Outlier Replacement

Pattern: Replace extreme returns with median return

Problem:

Tail events are often the most informative data points
Destroys fat-tail structure
Creates false impression of normality
Leads to severe risk underestimation
Example Impact:

Flash crash returns are "cleaned away"
VaR models calibrated on sanitized data
Risk management failures during next crash
3. Arbitrary Smoothing

Example: Moving average smoothing to "reduce noise"

Problem:

Introduces look-ahead bias (future data affects past)
Destroys autocorrelation structure
Creates spurious patterns
Violates causality
3.4 The Missing Data Dilemma
Three Philosophies:

A. Complete Case Analysis (Listwise Deletion)

Exclude any bar with missing/flagged data
Pros: Clean, unambiguous
Cons: Severe data loss, survivorship bias
B. Available Case Analysis (Pairwise Deletion)

Use all available data per calculation
Pros: Maximum data utilization
Cons: Inconsistent sample sizes, complex interpretation
C. Flagged Inclusion

Include all data, propagate quality flags
Pros: Researcher decides tolerance, transparent
Cons: Requires discipline to respect flags
Recommendation for Quant Research: Flagged Inclusion (Option C)

Implement quality tiers:

Tier 1 (Gold): No flags, complete bars, no anomalies
Tier 2 (Silver): Minor flags (e.g., volume spike), but structurally sound
Tier 3 (Bronze): Suspicious (extreme moves, gaps), use with caution
Tier 4 (Excluded): Critical errors, cannot be trusted
Downstream research queries data by tier:

Conservative research (execution, risk) → Tier 1 only
Exploratory research → Tier 1-2
Tail event research → All tiers, analyze separately
4. Preservation and Transparency
4.1 Multi-Layer Storage Architecture
Layer 1: Raw (Immutable)

Exactly as received from API
Never modified
Permanent record
Layer 2: Validated

Raw + quality flags
No corrections, only annotations
Schema:
  All raw columns +
  - has_ohlc_violation: bool
  - has_duplicate: bool
  - has_extreme_move: bool
  - has_volume_spike: bool
  - has_zero_volume: bool
  - gap_before_bars: int
  - gap_after_bars: int
  - quality_tier: int (1-4)
Layer 3: Cleaned

Safe corrections applied
Tier 4 data excluded
Schema:
  All validated columns +
  - corrections_applied: list[str]
  - original_price: float (if rounded)
Layer 4: Research-Ready

Feature-engineered datasets
Derived from Layer 3
Multiple versions for different research needs
Lineage Tracking:

Every dataset includes:

yaml
metadata:
  source_layer: "validated"
  parent_version: "v1.2"
  transformations:
    - "tick_size_rounding"
    - "timezone_utc_conversion"
  exclusions:
    - "quality_tier == 4"
  created_at: "2024-02-08T10:30:00Z"
  creator: "cleaning_pipeline_v2.3"
4.2 Quality Metrics & Reporting
Daily Quality Dashboard:

Per-Symbol Metrics:

Completeness Rate: (actual_bars / expected_bars) * 100%
Violation Rate: flagged_bars / total_bars
Exclusion Rate: tier_4_bars / total_bars
Gap Count: Number of temporal gaps
Anomaly Breakdown: Count by anomaly type
Cross-Sectional Metrics:

Correlation Matrix Stability: Compare to rolling 20-day average
Synchronized Availability: % of bars where all 30 symbols have data
Extreme Move Concordance: When one stock spikes, how many others do?
Temporal Metrics:

Intraday Quality Curve: Violation rate by minute-of-day
Regime-Conditional Quality: Quality during high vs. low volatility
Quality Autocorrelation: Do bad data days cluster?
Alert Triggers:

Any symbol with >5% tier 4 exclusions in single day
3 symbols with extreme moves in same minute (investigate correlation)

Any complete missing day not explained by calendar
Violation rate increases >2σ above historical average
4.3 Audit Trail & Reproducibility
Change Log:

Every cleaning run generates:

json
{
  "run_id": "clean_20240208_183000",
  "input_data": {
    "layer": "raw",
    "version": "v2024.02.08",
    "date_range": ["2024-02-01", "2024-02-08"],
    "symbols": ["AAPL", "MSFT", ...]
  },
  "pipeline_config": {
    "tick_rounding": true,
    "extreme_move_threshold": 0.10,
    "gap_tolerance": 2
  },
  "results": {
    "bars_processed": 327600,
    "bars_flagged": 1247,
    "bars_excluded": 89,
    "tier_distribution": {
      "tier_1": 324123,
      "tier_2": 3388,
      "tier_3": 0,
      "tier_4": 89
    }
  },
  "exclusions": [
    {
      "symbol": "XYZ",
      "timestamp": "2024-02-05T14:37:00Z",
      "reason": "OHLC_VIOLATION",
      "details": "high < close"
    },
    ...
  ]
}
```

**Reproducibility Guarantee:**

Given:
- Input data version
- Pipeline configuration file
- Random seeds (if any stochastic components)

**Can exactly reproduce:**
- All quality flags
- All exclusions
- Final cleaned dataset

**Versioning Strategy:**
```
data/
├── raw/v2024.02.08/
├── validated/v2024.02.08_pipeline_v2.3/
└── cleaned/v2024.02.08_pipeline_v2.3_tier1/
```

---

## 5. Rationale & Pitfalls

### 5.1 Why This Framework Matters

**1. Prevents Silent Model Degradation**

**Scenario:** Forward-filling halts creates phantom liquidity
- Model trains on "complete" data
- Deploys to production
- Tries to trade during actual halt → severe slippage
- **Root cause:** Cleaning masked reality

**This Framework:** Preserves halts → model learns correct behavior

**2. Enables Robust Risk Estimation**

**Scenario:** Outlier winsorization removes crash returns
- VaR calibrated on truncated distribution
- Underestimates tail risk by 50%+
- Firm fails stress test or experiences unexpected losses

**This Framework:** Preserves extremes → realistic risk estimates

**3. Supports Reproducible Research**

**Scenario:** Different researchers clean data differently
- Researcher A: Aggressive outlier removal
- Researcher B: Conservative preservation
- **Same strategy, different backtests, contradictory conclusions**

**This Framework:** Standardized tiers → consistent results

**4. Facilitates Regulatory Compliance**

**Requirement:** Audit trail from raw data to model output

**This Framework:** 
- Full lineage tracking
- Immutable raw layer
- Documented transformations
- Explanation for every exclusion

### 5.2 Common Pitfalls & Failure Modes

**Pitfall 1: Over-Automation**

**Mistake:** Automated pipeline excludes "anomalies" without human review

**Example:**
- Pipeline flags 5% move as extreme
- Auto-excludes
- **Actually was FDA drug approval** (legitimate signal)

**Mitigation:**
- Tier-based flagging (not automatic exclusion)
- Manual review queue for ambiguous cases
- News feed integration for context

**Pitfall 2: Inconsistent Treatment Across Time**

**Mistake:** Cleaning rules change mid-dataset

**Example:**
- 2020-2023: Keep extreme moves
- 2024: Exclude extreme moves
- **Result:** Strategy appears to work in 2020-2023, fails in 2024

**Mitigation:**
- Version cleaning pipeline
- Reprocess entire dataset when rules change
- Document methodology changes prominently

**Pitfall 3: Cross-Sectional Bias**

**Mistake:** Clean each symbol independently

**Example:**
- Stock A has bad data on June 15 → exclude
- Stock B has clean data on June 15 → include
- **Result:** June 15 universe is biased (missing A)

**Mitigation:**
- Synchronize exclusions across universe
- If any symbol in portfolio has tier 4 data on date → flag entire day
- Document "synchronized availability" metric

**Pitfall 4: Look-Ahead Contamination**

**Mistake:** Use future data to decide if current bar is anomaly

**Example:**
```
Bar at 10:30 shows +12% move
Check bar at 10:31: if reverses, flag as error
Problem: At 10:30, we don't know what 10:31 will be

Mitigation:

Only use strictly causal information for real-time flags
Can use future data for offline research flags, but document clearly
Pitfall 5: Ignoring Data Generating Process

Mistake: Apply time-series imputation methods designed for sensors/IoT to market data

Example: Kalman filter to interpolate missing bars

Problem:

Market data has non-stationary regime shifts
Gaps are informative (halts, liquidity crises)
Interpolation assumes smooth dynamics (invalid)
Mitigation:

Respect market microstructure
Imputation only for visualization, never for analysis
Preserve discrete nature of trading events
5.3 Trade-offs and Design Tensions
Tension 1: Data Quantity vs. Quality

Conservative Cleaning: High quality, low quantity Permissive Cleaning: High quantity, questionable quality

Resolution: Tier system allows both

Conservative researchers use tier 1 (smaller dataset, high quality)
Aggressive researchers use tier 1-2 (larger dataset, flagged quality)
Tension 2: Automation vs. Judgment

Full Automation: Scalable, consistent, but misses context Manual Review: Contextual, but subjective, slow

Resolution: Hybrid approach

Automate detection and flagging
Manual review for:
Ambiguous cases
High-impact decisions
Quality assurance sampling
Tension 3: Preservation vs. Usability

Maximum Preservation: Keep everything, complexity burden on researcher Heavy Cleaning: Easy to use, but loses information

Resolution: Multi-layer architecture

Layer 2 (Validated): Maximum preservation
Layer 3 (Cleaned): Research-ready, documented exclusions
Researcher chooses appropriate layer
6. Integration with Upstream and Downstream Processes
6.1 Upstream: Ingestion Layer Interface
Requirements from Ingestion:

Quality Metadata: Ingestion should flag:
Incomplete responses (requested 390 bars, received 387)
API errors during fetch
Timestamp discontinuities
Versioning: Ingestion must provide:
Data version ID
Fetch timestamp
Source configuration
Idempotency: Cleaning should produce identical output when re-run on same input
6.2 Downstream: Feature Engineering Interface
Guarantees Provided to Feature Engineering:

Schema Stability: Cleaned data has fixed schema
Temporal Ordering: Bars are strictly ordered by timestamp
Quality Flags: All bars include quality tier
No Look-Ahead: All metadata is strictly causal
Feature Engineering Responsibilities:

Respect Quality Tiers: Filter or weight by tier
Handle Gaps: Use gap-aware calculations (not forward-fill assumptions)
Propagate Flags: Derived features inherit quality concerns
6.3 Downstream: Model Training Interface
Critical Communication:

Dataset Characteristics Card:

yaml
dataset_id: "cleaned_tier1_v2024.02.08"
coverage:
  symbols: 30
  date_range: ["2020-01-01", "2024-02-08"]
  total_bars: 12,450,000
  excluded_bars: 45,231
  exclusion_rate: 0.36%
quality:
  tier_1_pct: 98.2%
  tier_2_pct: 1.4%
  tier_3_pct: 0.0%
  tier_4_pct: 0.4% (excluded)
limitations:
  - "TSLA 2020-03-15 incomplete (trading halt)"
  - "Market-wide data gap 2023-01-18 10:15-10:17 (NYSE issue)"
```

**Model training should:**
- Document which tiers were used
- Report sensitivity to tier selection
- Test model on tier 2 data (degraded quality)

---

## 7. Advanced Topics

### 7.1 Regime-Aware Cleaning

**Observation:** Anomaly thresholds should vary by volatility regime

**Example:**
- **Normal regime:** 5% move is suspicious
- **Crisis regime (VIX > 40):** 5% move is expected

**Implementation:**
- Estimate current regime (using HMM or rolling volatility)
- Apply regime-conditional thresholds
- Flag moves that are extreme **relative to regime**

### 7.2 Cross-Asset Consistency Checks

**Principle:** Related assets should exhibit correlated behavior

**Checks:**

1. **ETF-Constituent Consistency:**
   - SPY vs. average of S&P 500 components
   - Large divergence → investigate

2. **Pair Consistency:**
   - Cointegrated pairs (e.g., Coke vs. Pepsi)
   - Correlation breakdown → flag

3. **Sector Consistency:**
   - If 5/6 tech stocks spike, 6th flat → check 6th for data issue

### 7.3 Microstructure-Aware Cleaning

**Bid-Ask Bounce:**

**Pattern:** Price oscillates between bid and ask
```
100.01 (ask) → 100.00 (bid) → 100.01 (ask) → ...
Naive cleaning: Might flag as anomaly (high return volatility)

Informed cleaning: Recognize as microstructure noise, not error

Treatment:

Don't flag as anomaly
Provide mid-price estimates as alternative
Let researcher choose tick data vs. mid-price
7.4 Machine Learning for Anomaly Detection
Unsupervised Approaches:

Isolation Forest: Detect multivariate outliers in (return, volume, spread) space
Autoencoders: Learn normal bar patterns, flag high reconstruction errors
DBSCAN: Cluster bars, identify isolated points
Advantages:

Captures complex interactions
Adapts to data distribution
Risks:

Black box decisions
May flag genuine rare events
Requires validation against known anomalies
Recommendation: Use ML for flagging candidates, not automatic exclusion

Validation: Test on:

Known data errors (should detect)
Known market events (should NOT flag as errors)
8. Operational Considerations
8.1 Computational Efficiency
Challenge: Cleaning 30 symbols × 4 years × 390 bars/day = 13M+ bars

Strategies:

Incremental Processing:
Only clean new data
Cache validation results
Reuse flags from previous runs
Parallelization:
Clean symbols independently
Parallel validation checks
Distributed outlier detection
Lazy Evaluation:
Don't load entire dataset
Process in chunks
Stream validation results
8.2 Human-in-the-Loop Workflows
Review Queue System:

Tier 1: Auto-approve (clear data, no flags) Tier 2: Auto-approve with logging Tier 3: Queue for review (ambiguous cases) Tier 4: Auto-exclude (critical errors)

Review Interface:

Show flagged bar in context (surrounding bars, other symbols)
Display relevant news/events
Provide recommendation + confidence
Log decision + rationale
Build feedback loop (improve auto-classification)
8.3 Continuous Improvement
Feedback Mechanisms:

Model Performance Monitoring:
If model fails on specific dates → investigate data quality
Pattern of failures → systematic cleaning issue
Researcher Feedback:
Survey: "How often do you encounter data issues?"
Collect examples of missed anomalies
Prioritize fixes based on impact
Vendor Feedback:
Report systematic issues to data provider
Track vendor quality over time
Switch providers if quality degrades
9. Case Studies
Case Study 1: Flash Crash (May 6, 2010)
Scenario: Market drops 9% in minutes, recovers

Naive Cleaning: Flag as extreme outlier, exclude

Consequences:

Model never sees crash dynamics
Risk estimates severely understated
Trading system unprepared for future crashes
Proper Cleaning:

Preserve event fully
Flag as extreme_event=True
Enrich with metadata (CFTC report link, halt info)
Treat as crucial training data for tail risk
Lesson: Extreme events are features, not bugs

Case Study 2: Trading Halt Mishandling
Scenario: Stock halted 10:15-11:30

Naive Cleaning: Forward-fill prices during halt

Consequences:

Model thinks stock traded at $50 continuously
Learns to submit orders during halts
Production system faces reject orders, position tracking errors
Proper Cleaning:

Mark bars as trading_halt=True
Preserve gap
Downstream models learn: "no trading possible during halt"
Lesson: Gaps carry information

Case Study 3: Corporate Action Misalignment
Scenario: 2-for-1 split on June 15

Naive Cleaning: Clean pre-split and post-split data separately

Consequences:

June 14: Price = $100
June 15: Price = $50 (post-split)
Implied return: -50% (ERROR)
Proper Cleaning:

Apply adjustment to entire pre-split history
Verify adjustment factor (2.0)
Store both raw and adjusted prices
Document adjustment date in metadata
Lesson: Cross-temporal consistency requires holistic view

10. Quality Assurance Framework
10.1 Validation Tests
Unit Tests (Per Bar):

OHLC constraints satisfied
Timestamp in valid range
Volume ≥ 0
Price > 0
Tick size compliance
Integration Tests (Per Day):

Expected bar count ± tolerance
No duplicate timestamps
Monotonic timestamp ordering
Cross-symbol availability rates
System Tests (Full Dataset):

Return distribution matches empirical (pre-cleaning vs. post-cleaning)
Autocorrelation structure preserved
Volatility clustering maintained
No systematic bias introduced (mean return ≈ 0)
10.2 Regression Testing
Prevent Regressions:

When cleaning pipeline changes:

Run on historical "golden dataset"
Compare to previous version outputs
Flag unexpected differences
Require signoff on intentional changes
Golden Test Cases:

Known good day (no issues)
Flash crash day (extreme events)
Trading halt day
Early close day
High volatility day
Low liquidity day
10.3 Statistical Validation
Preserve Statistical Properties:

Before Cleaning vs. After Cleaning:

Return Distribution:
Mean ≈ same (should be ~0 for intraday)
Variance ≈ same
Skewness ≈ same
Kurtosis ≈ same
Large changes indicate over-cleaning
Autocorrelation:
ACF(returns) ≈ 0 before and after
ACF(|returns|) shows persistence before and after
ACF(returns²) shows volatility clustering before and after
Cross-Correlations:
Pairwise correlations preserved
Factor loadings unchanged
Alert if:

Kurtosis drops >20% (losing tail events)
Autocorrelation structure changes significantly
Mean shifts away from zero
11. Documentation Standards
11.1 Researcher-Facing Documentation
Data Quality Report (Per Dataset):

markdown
# Dataset: cleaned_tier1_v2024.02.08

## Summary
- Coverage: 30 symbols, 2020-01-01 to 2024-02-08
- Total bars: 12,404,769
- Excluded: 45,231 (0.36%)
- Quality tier 1: 98.2%

## Known Issues
1. TSLA: 2020-03-15 incomplete (trading halt)
2. All symbols: 2023-01-18 10:15-10:17 gap (NYSE technical issue)
3. NVDA: 2024-01-05 extreme volatility (earnings release)

## Cleaning Methodology
- Pipeline version: v2.3
- Extreme move threshold: 10%
- Tick rounding: Enabled
- Exclusion criteria: OHLC violations, duplicate timestamps

## Usage Recommendations
- **For execution modeling:** Use tier 1 only
- **For volatility research:** Tiers 1-2 acceptable
- **For tail risk:** Include tier 3 (review flagged events)

## Validation Results
- Return distribution: ✓ Matches empirical
- Autocorrelation: ✓ Preserved
- Kurtosis: 8.2 (empirical: 8.5) ✓
11.2 Internal Technical Documentation
Pipeline Configuration:

yaml
cleaning_pipeline:
  version: "2.3"
  input: "raw/v2024.02.08"
  output: "cleaned/v2024.02.08_tier1"
  
  rules:
    ohlc_validation:
      enabled: true
      action: "exclude"
    
    extreme_moves:
      threshold: 0.10
      action: "flag"
      tier: 2
    
    tick_rounding:
      enabled: true
      threshold: 0.01  # for prices >= $1
    
    gap_handling:
      max_gap: 2
      action: "preserve_with_flag"
  
  exclusions:
    - ohlc_violations
    - duplicate_timestamps_conflicting
  
  tiers:
    tier_1: [no_flags]
    tier_2: [extreme_move, volume_spike, zero_volume]
    tier_3: [needs_review]
    tier_4: [critical_error]
    
12. Conclusion
Data cleaning in quantitative finance is fundamentally different from traditional data science. The stakes are higher, the data is more complex, and the consequences of errors are severe. This framework prioritizes:

Transparency: Every decision is logged and auditable
Conservatism: Preserve reality, even when messy
Flexibility: Tier system supports different research needs
Reproducibility: Version everything, document all changes
Statistical Rigor: Validate that cleaning preserves essential properties
The Golden Rule: When in doubt, flag but don't exclude. Let the researcher decide.

Bad data with clear warnings is better than corrupted data that appears clean. The goal is not a perfect dataset—perfect datasets don't exist in financial markets. The goal is a trustworthy dataset where every imperfection is documented, every exclusion is justified, and every researcher can make informed decisions about data quality trade-offs.

This foundation enables robust downstream research: features that reflect reality, models that generalize, and risk estimates that protect capital.

